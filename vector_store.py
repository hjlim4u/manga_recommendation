from typing import List, Optional
import logging
import re
import numpy as np
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Qdrant
from langchain_core.documents import Document
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, Filter, FieldCondition, MatchValue, MatchAny, PayloadSchemaType
from data_source import MangaDataSource
import os
from dotenv import load_dotenv

load_dotenv()

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

class QdrantMangaStore:
    """Qdrant ë²¡í„° ì €ì¥ì†Œë¥¼ ì‚¬ìš©í•œ ë§Œí™” ë°ì´í„° ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, collection_name: str = "manga_collection"):
        self.collection_name = collection_name
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
        
        # Qdrant í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        # í´ë¼ìš°ë“œ Qdrant ì„¤ì •
        self.client = QdrantClient(
            url=os.getenv("QDRANT_URL"),
            api_key=os.getenv("QDRANT_API_KEY"),
            prefer_grpc=True,
            timeout=20000
        )
        
        # ì»¬ë ‰ì…˜ ìƒì„±
        self._create_collection_if_not_exists()

        # LangChain Qdrant ë˜í¼
        self.vector_store = Qdrant(
            client=self.client,
            collection_name=self.collection_name,
            embeddings=self.embeddings
        )
    
    def _create_collection_if_not_exists(self):
        """ì»¬ë ‰ì…˜ì´ ì—†ìœ¼ë©´ ìƒì„±"""
        collections = self.client.get_collections().collections
        if not any(col.name == self.collection_name for col in collections):
            self.client.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(
                    size=3072,  # text-embedding-3-large ì°¨ì›
                    distance=Distance.COSINE
                )
            )
            logger.info(f"Created collection: {self.collection_name}")

            # ì œëª© í•„ë“œë“¤ì— ëŒ€í•œ keyword ì¸ë±ìŠ¤ ìƒì„±
            title_fields = ['metadata.title', 'metadata.title_english', 'metadata.title_japanese', 'metadata.status', 'metadata.demographics', 'metadata.genres',"metadata.manga_id"]
            for field in title_fields:
                try:
                    self.client.create_payload_index(
                        collection_name=self.collection_name,
                        field_name=field,
                        field_type=PayloadSchemaType.KEYWORD if field != "metadata.manga_id" else PayloadSchemaType.INTEGER
                    )
                    logger.info(f"Created keyword index for: {field}")
                except Exception as e:
                    logger.warning(f"Failed to create index for {field}: {e}")

    def is_collection_empty(self) -> bool:
        """ì»¬ë ‰ì…˜ì´ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸"""
        try:
            collection_info = self.client.get_collection(self.collection_name)
            return collection_info.points_count == 0
        except Exception:
            return True
    
    def delete_collection(self):
        """ì»¬ë ‰ì…˜ ì‚­ì œí•˜ê³  ìƒˆë¡œ ìƒì„±"""
        try:
            self.client.delete_collection(self.collection_name)
            logger.info(f"ì»¬ë ‰ì…˜ '{self.collection_name}' ì‚­ì œë¨")
        except Exception:
            pass
        
        self._create_collection_if_not_exists()
        # LangChain ë˜í¼ ì¬ì´ˆê¸°í™”
        self.vector_store = Qdrant(
            client=self.client,
            collection_name=self.collection_name,
            embeddings=self.embeddings
        )
    
    def load_and_index_from_source(self, data_source: MangaDataSource, source_batch_size: int = 1000, index_batch_size: int = 100, force_reindex: bool = False):
        """ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ë§Œí™” ë°ì´í„°ë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë¡œë“œí•˜ê³  ë²¡í„° DBì— ì¸ë±ì‹± (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )"""
        
        # ë°ì´í„°ê°€ ì´ë¯¸ ìˆê³  ê°•ì œ ì¬ì¸ë±ì‹±ì´ ì•„ë‹ˆë©´ ê±´ë„ˆë›°ê¸°
        if not force_reindex and not self.is_collection_empty():
            logger.info("âœ… ì»¬ë ‰ì…˜ì— ì´ë¯¸ ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤. ì¸ë±ì‹±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        #
        total_count = data_source.get_total_count()
        logger.info(f"Starting streaming indexing from data source (Total: {total_count} records)")
        
        processed_count = 0
        batch_count = 0
        
        try:
            # ë°°ì¹˜ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°ì´í„° ì²˜ë¦¬
            for manga_batch in data_source.load_manga_data_batches(source_batch_size):
                batch_count += 1
                
                # Document ê°ì²´ë¡œ ë³€í™˜
                documents = data_source.create_documents(manga_batch)
                logger.info(f"Batch {batch_count}: Created {len(documents)} documents")
                
                # ë²¡í„° DBì— ì¸ë±ì‹±
                self.index_manga_batch(documents, index_batch_size)
                
                processed_count += len(documents)
                logger.info(f"Progress: {processed_count}/{total_count} records indexed ({processed_count/total_count*100:.1f}%)")
                
                # ë©”ëª¨ë¦¬ ì ˆì•½: ë°°ì¹˜ ì²˜ë¦¬ í›„ ì¦‰ì‹œ í•´ì œ
                del manga_batch, documents
            
            logger.info(f"Streaming indexing completed successfully: {processed_count} records in {batch_count} batches")
            
        except Exception as e:
            logger.error(f"Streaming indexing failed: {e}")
            raise
    
    def index_manga_batch(self, documents: List[Document], batch_size: int = 100):
        """ëŒ€ëŸ‰ì˜ ë§Œí™” ë°ì´í„°ë¥¼ ë°°ì¹˜ë¡œ ì¸ë±ì‹± (ê°„ì†Œí™”ëœ ë²„ì „)"""
        total = len(documents)
        logger.info(f"Starting batch indexing of {total} documents")

        for i in range(0, total, batch_size):
            batch = documents[i:i + batch_size]
            # ğŸ¯ ì²« ë²ˆì§¸ ë¬¸ì„œì˜ ì„ë² ë”© ê°’ì„ ì½˜ì†”ì— ì¶œë ¥
            if batch:
                first_doc = batch[0]
                print(f"\n=== ë°°ì¹˜ {i // batch_size + 1} ì²« ë²ˆì§¸ ë¬¸ì„œ ì„ë² ë”© ===")
                print(f"ë¬¸ì„œ ì œëª©: {first_doc.metadata.get('title', 'N/A')}")
                print(f"ë¬¸ì„œ ë‚´ìš© (ì• 100ì): {first_doc.page_content[:100]}...")

            # ğŸ¯ ë³€í™˜ ì‘ì—… ì—†ì´ ë°”ë¡œ ì¸ë±ì‹±
            self.vector_store.add_documents(batch)
            logger.info(f"Indexed {min(i + batch_size, total)}/{total} documents")

    def find_manga_by_title(self, target_title: str) -> Optional[Document]:
        """ğŸš€ ì œëª© ê²€ìƒ‰ (ìƒˆë¡œìš´ í˜•ì‹ ì§€ì›: title, title_english, title_japanese)"""
        try:
            logger.info(f"ğŸ” ì œëª© ê²€ìƒ‰: '{target_title}'")

            # 1ë‹¨ê³„: ì—¬ëŸ¬ ì œëª© í•„ë“œì—ì„œ ì •í™• ë§¤ì¹­ ì‹œë„
            title_fields = ['metadata.title', 'metadata.title_english', 'metadata.title_japanese']

            for field in title_fields:
                search_result = self.client.scroll(
                    collection_name=self.collection_name,
                    scroll_filter=Filter(
                        must=[FieldCondition(
                            key=field,
                            match=MatchValue(value=target_title)
                        )]
                    ),
                    limit=1,
                    with_payload=True,
                    with_vectors=False
                )

                if search_result[0]:  # ì •í™• ë§¤ì¹­ ì„±ê³µ
                    point = search_result[0][0]
                    matched_title = point.payload['metadata'].get(field.split('.')[-1], '')
                    logger.info(f"âœ… ì •í™• ë§¤ì¹­ ({field}): '{target_title}' â†’ '{matched_title}'")
                    return Document(
                        page_content=point.payload['page_content'],
                        metadata=point.payload['metadata']
                    )

            # 2ë‹¨ê³„: ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
            logger.info(f"ì •í™• ë§¤ì¹­ ì‹¤íŒ¨, ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹œë„...")

            # ì œëª©ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
            title_embedding = self.embeddings.embed_query(target_title)

            # ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ê°€ì¥ ìœ ì‚¬í•œ ë§Œí™” ì°¾ê¸°
            search_results = self.client.query_points(
                collection_name=self.collection_name,
                query=title_embedding,
                limit=1,
                with_payload=True,
                with_vectors=False,
                score_threshold=0.1
            )

            if search_results.points:
                point = search_results.points[0]
                db_title = point.payload['metadata']['title']
                logger.info(f"âœ… ìœ ì‚¬ë„ ê²€ìƒ‰: '{target_title}' â†’ '{db_title}' (ì ìˆ˜: {point.score:.3f})")

                return Document(
                    page_content=point.payload['page_content'],
                    metadata=point.payload['metadata']
                )

            logger.warning(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: '{target_title}'")
            return None

        except Exception as e:
            logger.error(f"ì œëª© ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return None

    def search_similar_manga_by_centroid(self, favorite_manga_docs: List[Document],
                                       preferred_genres: List[str], demographic: str,
                                       limit: int = 30) -> List[Document]:
        """ì¤‘ì‹¬ì  ì„ë² ë”©ì„ ì‚¬ìš©í•œ ìœ ì‚¬ ë§Œí™” ê²€ìƒ‰ (ìƒˆë¡œìš´ í˜•ì‹ ì§€ì›)"""
        try:
            if not favorite_manga_docs:
                logger.warning("ì¢‹ì•„í•˜ëŠ” ë§Œí™”ê°€ ì—†ì–´ì„œ ì¤‘ì‹¬ì  ê²€ìƒ‰ ë¶ˆê°€ëŠ¥")
                return []

            exclude_ids = [doc.metadata['manga_id'] for doc in favorite_manga_docs]
            logger.info(f"ì¤‘ì‹¬ì  ê²€ìƒ‰ - ì¢‹ì•„í•˜ëŠ” ë§Œí™” {len(favorite_manga_docs)}ê°œ, ì œì™¸ ID: {exclude_ids}")

            # ì¤‘ì‹¬ì  ì„ë² ë”© ìƒì„±
            embeddings = []
            for doc in favorite_manga_docs:
                embedding = self.embeddings.embed_query(doc.page_content)
                embeddings.append(embedding)

            centroid = np.mean(embeddings, axis=0)
            centroid = centroid / np.linalg.norm(centroid)

            # ê²€ìƒ‰ í•„í„° êµ¬ì„± (ìƒˆë¡œìš´ í˜•ì‹ ì§€ì›)
            search_filter = self._build_search_filter(preferred_genres, demographic, exclude_ids)

            # ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰
            search_result = self.client.query_points(
                collection_name=self.collection_name,
                query=centroid.tolist(),
                query_filter=search_filter,
                limit=limit,
                with_payload=True,
                with_vectors=False
            )

            logger.info(f"ì¤‘ì‹¬ì  ê²€ìƒ‰ ê²°ê³¼: {len(search_result.points)}ê°œ")

            # Document ë³€í™˜
            documents = []
            for point in search_result.points:
                metadata = point.payload.get('metadata', {}).copy()
                metadata['similarity_score'] = point.score

                doc = Document(
                    page_content=point.payload.get('page_content', ''),
                    metadata=metadata
                )
                documents.append(doc)

            return documents

        except Exception as e:
            logger.error(f"ì¤‘ì‹¬ì  ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def search_similar_manga_by_individual(self, favorite_manga_docs: List[Document],
                                         preferred_genres: List[str], demographic: str,
                                         limit_per_manga: int = 15) -> List[Document]:
        """ê°œë³„ ë§Œí™” ê²€ìƒ‰ í›„ ë³‘í•© (ìƒˆë¡œìš´ í˜•ì‹ ì§€ì›)"""
        try:
            if not favorite_manga_docs:
                logger.warning("ì¢‹ì•„í•˜ëŠ” ë§Œí™”ê°€ ì—†ì–´ì„œ ê°œë³„ ê²€ìƒ‰ ë¶ˆê°€ëŠ¥")
                return []

            exclude_ids = [doc.metadata['manga_id'] for doc in favorite_manga_docs]
            logger.info(f"ê°œë³„ ê²€ìƒ‰ - ì¢‹ì•„í•˜ëŠ” ë§Œí™” {len(favorite_manga_docs)}ê°œ")

            all_results = {}

            for target_doc in favorite_manga_docs:
                # ê°œë³„ ì„ë² ë”©ìœ¼ë¡œ ê²€ìƒ‰
                embedding = self.embeddings.embed_query(target_doc.page_content)

                # ê²€ìƒ‰ í•„í„° êµ¬ì„±
                search_filter = self._build_search_filter(preferred_genres, demographic, exclude_ids)

                # ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰
                search_result = self.client.query_points(
                    collection_name=self.collection_name,
                    query=embedding,
                    query_filter=search_filter,
                    limit=limit_per_manga
                )

                logger.info(f"'{target_doc.metadata['title']}' ê²€ìƒ‰ ê²°ê³¼: {len(search_result.points)}ê°œ")

                # ì ìˆ˜ ëˆ„ì 
                for point in search_result.points:
                    manga_id = point.payload['metadata']['manga_id']
                    if manga_id not in all_results:
                        all_results[manga_id] = {
                            'point': point,
                            'total_score': 0,
                            'count': 0
                        }
                    all_results[manga_id]['total_score'] += point.score
                    all_results[manga_id]['count'] += 1

            # í‰ê·  ì ìˆ˜ë¡œ ì •ë ¬
            sorted_results = sorted(
                all_results.values(),
                key=lambda x: x['total_score'] / x['count'],
                reverse=True
            )

            logger.info(f"ê°œë³„ ê²€ìƒ‰ ë³‘í•© ê²°ê³¼: {len(sorted_results)}ê°œ")

            # Document ë³€í™˜
            documents = []
            for item in sorted_results[:30]:  # ìƒìœ„ 30ê°œë§Œ
                point = item['point']
                metadata = point.payload.get('metadata', {}).copy()
                metadata['similarity_score'] = item['total_score'] / item['count']

                doc = Document(
                    page_content=point.payload.get('page_content', ''),
                    metadata=metadata
                )
                documents.append(doc)

            return documents

        except Exception as e:
            logger.error(f"ê°œë³„ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def _build_search_filter(self, preferred_genres: List[str], demographic: str,
                           exclude_ids: List[int]) -> Filter:
        """ê²€ìƒ‰ í•„í„° êµ¬ì„± (ìƒˆë¡œìš´ í˜•ì‹ ì§€ì›)"""
        # í•„ìˆ˜ ì¡°ê±´
        # must_conditions = [
        #     FieldCondition(
        #         key="metadata.demographics",
        #         match=MatchAny(any=[demographic])
        #     )
        # ]

        # # ì¥ë¥´ í•„í„°ë§ (ì„ íƒì )
        # if preferred_genres:
        #     # ìƒˆë¡œìš´ í˜•ì‹ì—ì„œëŠ” genres ë°°ì—´ ë˜ëŠ” main_genre_cd_nmìœ¼ë¡œ í•„í„°ë§
        #     must_conditions.append(
        #         FieldCondition(
        #             key="metadata.main_genre_cd_nm",
        #             match=MatchAny(any=preferred_genres)
        #         )
        #     )

        # ì œì™¸ ì¡°ê±´
        must_not_conditions = []
        if exclude_ids:
            for exclude_id in exclude_ids:
                must_not_conditions.append(
                    FieldCondition(
                        key="metadata.manga_id",
                        match=MatchValue(value=exclude_id),
                    )
                )

        return Filter(
            # must=must_conditions,
            must_not=must_not_conditions if must_not_conditions else None
        )

    def debug_vector_db_contents(self, limit: int = 10):
        """ğŸ” ë²¡í„° DB ë‚´ìš© ë””ë²„ê¹… (ìƒˆë¡œìš´ í˜•ì‹ ì§€ì›)"""
        try:
            # ë²¡í„° DBì—ì„œ ìƒ˜í”Œ ê°€ì ¸ì˜¤ê¸°
            search_result = self.client.scroll(
                collection_name=self.collection_name,
                limit=limit,
                with_payload=True,
                with_vectors=False
            )

            logger.info(f"ğŸ” ë²¡í„° DB ìƒ˜í”Œ ì œëª©ë“¤ ({len(search_result[0])}ê°œ):")
            for i, point in enumerate(search_result[0]):
                metadata = point.payload.get('metadata', {})
                title = metadata.get('title', 'N/A')
                title_english = metadata.get('title_english', 'N/A')
                title_japanese = metadata.get('title_japanese', 'N/A')
                manga_id = metadata.get('manga_id', 'N/A')
                status = metadata.get('status', 'N/A')
                genres = metadata.get('genres', [])
                logger.info(f"  {i+1}. ID:{manga_id} | title:'{title}' | english:'{title_english}' | japanese:'{title_japanese}' | status:{status} | genres:{genres}")

        except Exception as e:
            logger.error(f"ë²¡í„° DB ë””ë²„ê¹… ì‹¤íŒ¨: {e}")