from typing import List, Optional
import logging
import re
import numpy as np
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Qdrant
from langchain_core.documents import Document
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, Filter, FieldCondition, MatchValue, MatchAny, Range
from domain import AgeRating
from data_source import MangaDataSource

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

class QdrantMangaStore:
    """Qdrant ë²¡í„° ì €ì¥ì†Œë¥¼ ì‚¬ìš©í•œ ë§Œí™” ë°ì´í„° ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, collection_name: str = "manga_collection"):
        self.collection_name = collection_name
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
        
        # Qdrant í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = QdrantClient(host="localhost", port=6333)
        
        # ì»¬ë ‰ì…˜ ìƒì„±
        self._create_collection_if_not_exists()
        
        # LangChain Qdrant ë˜í¼
        self.vector_store = Qdrant(
            client=self.client,
            collection_name=self.collection_name,
            embeddings=self.embeddings
        )
    
    def _create_collection_if_not_exists(self):
        """ì»¬ë ‰ì…˜ì´ ì—†ìœ¼ë©´ ìƒì„±"""
        collections = self.client.get_collections().collections
        if not any(col.name == self.collection_name for col in collections):
            self.client.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(
                    size=3072,  # text-embedding-3-large ì°¨ì›
                    distance=Distance.COSINE
                )
            )
            logger.info(f"Created collection: {self.collection_name}")
    
    def is_collection_empty(self) -> bool:
        """ì»¬ë ‰ì…˜ì´ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸"""
        try:
            collection_info = self.client.get_collection(self.collection_name)
            return collection_info.points_count == 0
        except Exception:
            return True
    
    def delete_collection(self):
        """ì»¬ë ‰ì…˜ ì‚­ì œí•˜ê³  ìƒˆë¡œ ìƒì„±"""
        try:
            self.client.delete_collection(self.collection_name)
            logger.info(f"ì»¬ë ‰ì…˜ '{self.collection_name}' ì‚­ì œë¨")
        except Exception:
            pass
        
        self._create_collection_if_not_exists()
        # LangChain ë˜í¼ ì¬ì´ˆê¸°í™”
        self.vector_store = Qdrant(
            client=self.client,
            collection_name=self.collection_name,
            embeddings=self.embeddings
        )
    
    def load_and_index_from_source(self, data_source: MangaDataSource, source_batch_size: int = 1000, index_batch_size: int = 100, force_reindex: bool = False):
        """ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ë§Œí™” ë°ì´í„°ë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë¡œë“œí•˜ê³  ë²¡í„° DBì— ì¸ë±ì‹± (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )"""
        
        # ë°ì´í„°ê°€ ì´ë¯¸ ìˆê³  ê°•ì œ ì¬ì¸ë±ì‹±ì´ ì•„ë‹ˆë©´ ê±´ë„ˆë›°ê¸°
        if not force_reindex and not self.is_collection_empty():
            logger.info("âœ… ì»¬ë ‰ì…˜ì— ì´ë¯¸ ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤. ì¸ë±ì‹±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        
        total_count = data_source.get_total_count()
        logger.info(f"Starting streaming indexing from data source (Total: {total_count} records)")
        
        processed_count = 0
        batch_count = 0
        
        try:
            # ë°°ì¹˜ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°ì´í„° ì²˜ë¦¬
            for manga_batch in data_source.load_manga_data_batches(source_batch_size):
                batch_count += 1
                
                # Document ê°ì²´ë¡œ ë³€í™˜
                documents = data_source.create_documents(manga_batch)
                logger.info(f"Batch {batch_count}: Created {len(documents)} documents")
                
                # ë²¡í„° DBì— ì¸ë±ì‹±
                self.index_manga_batch(documents, index_batch_size)
                
                processed_count += len(documents)
                logger.info(f"Progress: {processed_count}/{total_count} records indexed ({processed_count/total_count*100:.1f}%)")
                
                # ë©”ëª¨ë¦¬ ì ˆì•½: ë°°ì¹˜ ì²˜ë¦¬ í›„ ì¦‰ì‹œ í•´ì œ
                del manga_batch, documents
            
            logger.info(f"Streaming indexing completed successfully: {processed_count} records in {batch_count} batches")
            
        except Exception as e:
            logger.error(f"Streaming indexing failed: {e}")
            raise
    
    def index_manga_batch(self, documents: List[Document], batch_size: int = 100):
        """ëŒ€ëŸ‰ì˜ ë§Œí™” ë°ì´í„°ë¥¼ ë°°ì¹˜ë¡œ ì¸ë±ì‹±"""
        total = len(documents)
        logger.info(f"Starting batch indexing of {total} documents")
        
        for i in range(0, total, batch_size):
            batch = documents[i:i + batch_size]
            
            for doc in batch:
                # ì—°ë ¹ ë“±ê¸‰ ê°’ ì¶”ê°€
                age_rating = AgeRating.from_option(
                    doc.metadata.get('age_grad_cd_nm', 'ì „ì²´ì—°ë ¹')
                )
                doc.metadata['age_rating_value'] = age_rating.value
                
                # ID ì •ìˆ˜ ë³€í™˜
                if 'id' in doc.metadata:
                    doc.metadata['manga_id'] = int(doc.metadata['id'])
            
            self.vector_store.add_documents(batch)
            logger.info(f"Indexed {min(i + batch_size, total)}/{total} documents")
    
    def find_manga_by_title(self, target_title: str) -> Optional[Document]:
        """ğŸš€ ê°„ë‹¨í•œ ì œëª© ê²€ìƒ‰: ê°€ì¥ ìœ ì‚¬í•œ ë§Œí™” 1ê°œ ë°˜í™˜"""
        try:
            logger.info(f"ğŸ” ì œëª© ê²€ìƒ‰: '{target_title}'")
            
            # 1ë‹¨ê³„: ì •í™• ë§¤ì¹­ ì‹œë„
            search_result = self.client.scroll(
                collection_name=self.collection_name,
                scroll_filter=Filter(
                    must=[FieldCondition(
                        key="metadata.title",
                        match=MatchValue(value=target_title)
                    )]
                ),
                limit=1,
                with_payload=True,
                with_vectors=False
            )
            
            if search_result[0]:  # ì •í™• ë§¤ì¹­ ì„±ê³µ
                point = search_result[0][0]
                logger.info(f"âœ… ì •í™• ë§¤ì¹­: '{target_title}'")
                return Document(
                    page_content=point.payload['page_content'],
                    metadata=point.payload['metadata']
                )
            
            # 2ë‹¨ê³„: ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ìœ¼ë¡œ ê°€ì¥ ìœ ì‚¬í•œ ë§Œí™” 1ê°œ ì°¾ê¸°
            logger.info(f"ì •í™• ë§¤ì¹­ ì‹¤íŒ¨, ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹œë„...")
            
            # ì œëª©ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
            title_embedding = self.embeddings.embed_query(target_title)
            
            # ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ê°€ì¥ ìœ ì‚¬í•œ ë§Œí™” ì°¾ê¸°
            search_results = self.client.query_points(
                collection_name=self.collection_name,
                query=title_embedding,
                limit=1,  # ê°€ì¥ ìœ ì‚¬í•œ 1ê°œë§Œ
                with_payload=True,
                with_vectors=False,
                score_threshold=0.1  # ë§¤ìš° ë‚®ì€ ì„ê³„ê°’ (ê±°ì˜ ëª¨ë“  ê²°ê³¼ í—ˆìš©)
            )
            
            if search_results.points:
                point = search_results.points[0]
                db_title = point.payload['metadata']['title']
                logger.info(f"âœ… ìœ ì‚¬ë„ ê²€ìƒ‰: '{target_title}' â†’ '{db_title}' (ì ìˆ˜: {point.score:.3f})")
                
                return Document(
                    page_content=point.payload['page_content'],
                    metadata=point.payload['metadata']
                )
            
            logger.warning(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: '{target_title}'")
            return None
            
        except Exception as e:
            logger.error(f"ì œëª© ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return None
    
    def search_similar_manga_by_centroid(self, favorite_manga_docs: List[Document], 
                                       preferred_genres: List[str], max_age_rating: int, 
                                       limit: int = 30) -> List[Document]:
        """ì¤‘ì‹¬ì  ì„ë² ë”©ì„ ì‚¬ìš©í•œ ìœ ì‚¬ ë§Œí™” ê²€ìƒ‰"""
        try:
            if not favorite_manga_docs:
                logger.warning("ì¢‹ì•„í•˜ëŠ” ë§Œí™”ê°€ ì—†ì–´ì„œ ì¤‘ì‹¬ì  ê²€ìƒ‰ ë¶ˆê°€ëŠ¥")
                return []
            
            exclude_ids = [doc.metadata['manga_id'] for doc in favorite_manga_docs]
            logger.info(f"ì¤‘ì‹¬ì  ê²€ìƒ‰ - ì¢‹ì•„í•˜ëŠ” ë§Œí™” {len(favorite_manga_docs)}ê°œ, ì œì™¸ ID: {exclude_ids}")
            
            # ì¤‘ì‹¬ì  ì„ë² ë”© ìƒì„±
            embeddings = []
            for doc in favorite_manga_docs:
                embedding = self.embeddings.embed_query(doc.page_content)
                embeddings.append(embedding)
            
            centroid = np.mean(embeddings, axis=0)
            centroid = centroid / np.linalg.norm(centroid)
            
            # ê²€ìƒ‰ í•„í„° êµ¬ì„±
            search_filter = self._build_search_filter(preferred_genres, max_age_rating, exclude_ids)
            
            # ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰
            search_result = self.client.query_points(
                collection_name=self.collection_name,
                query=centroid.tolist(),
                query_filter=search_filter,
                limit=limit,
                with_payload=True,
                with_vectors=False
            )
            
            logger.info(f"ì¤‘ì‹¬ì  ê²€ìƒ‰ ê²°ê³¼: {len(search_result.points)}ê°œ")
            
            # Document ë³€í™˜
            documents = []
            for point in search_result.points:
                metadata = point.payload.get('metadata', {}).copy()
                metadata['similarity_score'] = point.score
                
                doc = Document(
                    page_content=point.payload.get('page_content', ''),
                    metadata=metadata
                )
                documents.append(doc)
            
            return documents
            
        except Exception as e:
            logger.error(f"ì¤‘ì‹¬ì  ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def search_similar_manga_by_individual(self, favorite_manga_docs: List[Document], 
                                         preferred_genres: List[str], max_age_rating: int, 
                                         limit_per_manga: int = 15) -> List[Document]:
        """ê°œë³„ ë§Œí™” ê²€ìƒ‰ í›„ ë³‘í•©"""
        try:
            if not favorite_manga_docs:
                logger.warning("ì¢‹ì•„í•˜ëŠ” ë§Œí™”ê°€ ì—†ì–´ì„œ ê°œë³„ ê²€ìƒ‰ ë¶ˆê°€ëŠ¥")
                return []
            
            exclude_ids = [doc.metadata['manga_id'] for doc in favorite_manga_docs]
            logger.info(f"ê°œë³„ ê²€ìƒ‰ - ì¢‹ì•„í•˜ëŠ” ë§Œí™” {len(favorite_manga_docs)}ê°œ")
            
            all_results = {}
            
            for target_doc in favorite_manga_docs:
                # ê°œë³„ ì„ë² ë”©ìœ¼ë¡œ ê²€ìƒ‰
                embedding = self.embeddings.embed_query(target_doc.page_content)
                
                # ê²€ìƒ‰ í•„í„° êµ¬ì„±
                search_filter = self._build_search_filter(preferred_genres, max_age_rating, exclude_ids)
                
                # ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰
                search_result = self.client.query_points(
                    collection_name=self.collection_name,
                    query=embedding,
                    query_filter=search_filter,
                    limit=limit_per_manga
                )
                
                logger.info(f"'{target_doc.metadata['title']}' ê²€ìƒ‰ ê²°ê³¼: {len(search_result.points)}ê°œ")
                
                # ì ìˆ˜ ëˆ„ì 
                for point in search_result.points:
                    manga_id = point.payload['metadata']['manga_id']
                    if manga_id not in all_results:
                        all_results[manga_id] = {
                            'point': point,
                            'total_score': 0,
                            'count': 0
                        }
                    all_results[manga_id]['total_score'] += point.score
                    all_results[manga_id]['count'] += 1
            
            # í‰ê·  ì ìˆ˜ë¡œ ì •ë ¬
            sorted_results = sorted(
                all_results.values(),
                key=lambda x: x['total_score'] / x['count'],
                reverse=True
            )
            
            logger.info(f"ê°œë³„ ê²€ìƒ‰ ë³‘í•© ê²°ê³¼: {len(sorted_results)}ê°œ")
            
            # Document ë³€í™˜
            documents = []
            for item in sorted_results[:30]:  # ìƒìœ„ 30ê°œë§Œ
                point = item['point']
                metadata = point.payload.get('metadata', {}).copy()
                metadata['similarity_score'] = item['total_score'] / item['count']
                
                doc = Document(
                    page_content=point.payload.get('page_content', ''),
                    metadata=metadata
                )
                documents.append(doc)
            
            return documents
            
        except Exception as e:
            logger.error(f"ê°œë³„ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def _build_search_filter(self, preferred_genres: List[str], max_age_rating: int, 
                           exclude_ids: List[int]) -> Filter:
        """ê²€ìƒ‰ í•„í„° êµ¬ì„±"""
        # í•„ìˆ˜ ì¡°ê±´
        must_conditions = [
            # FieldCondition(
            #     key="metadata.main_genre_cd_nm",
            #     match=MatchAny(any=preferred_genres)
            # ),
            FieldCondition(
                key="metadata.age_rating_value",
                range=Range(lte=max_age_rating)
            )
        ]
        
        # ì œì™¸ ì¡°ê±´
        must_not_conditions = []
        if exclude_ids:
            for exclude_id in exclude_ids:
                must_not_conditions.append(
                    FieldCondition(
                        key="metadata.manga_id",
                        match=MatchValue(value=exclude_id),
                    )
                )
        
        return Filter(
            must=must_conditions,
            must_not=must_not_conditions if must_not_conditions else None
        )
    
    def debug_vector_db_contents(self, limit: int = 10):
        """ğŸ” ë²¡í„° DB ë‚´ìš© ë””ë²„ê¹… - ìƒì„¸ ì •ë³´ í¬í•¨"""
        try:
            # ë²¡í„° DBì—ì„œ ìƒ˜í”Œ ê°€ì ¸ì˜¤ê¸° (scroll ì‚¬ìš©)
            search_result = self.client.scroll(
                collection_name=self.collection_name,
                limit=limit,
                with_payload=True,
                with_vectors=False
            )
            
            logger.info(f"ğŸ” ë²¡í„° DB ìƒ˜í”Œ ì œëª©ë“¤ ({len(search_result[0])}ê°œ):")
            for i, point in enumerate(search_result[0]):
                metadata = point.payload.get('metadata', {})
                title = metadata.get('title', 'N/A')
                full_title = metadata.get('full_title', 'N/A') 
                manga_id = metadata.get('manga_id', 'N/A')
                logger.info(f"  {i+1}. ID:{manga_id} | title:'{title}' | full_title:'{full_title}'")
                
        except Exception as e:
            logger.error(f"ë²¡í„° DB ë””ë²„ê¹… ì‹¤íŒ¨: {e}") 